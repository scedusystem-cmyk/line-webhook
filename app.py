# app.py
# ============================================
# ã€Šå¯„æ›¸ï¼‹é€²éŠ·å­˜ è‡ªå‹•åŒ–æ©Ÿå™¨äººã€‹â€” å®Œæ•´ç‰ˆï¼ˆç™½åå–®ï¼åªå›è¦†æŒ‡ä»¤ï¼OCRé–€æª»ï¼å…¥åº«æ”¯æ´è² æ•¸ï¼å¤šæ›¸å±•é–‹ï¼å–æ¶ˆå¯„æ›¸ä¿®è£œï¼‰
# æ¶æ§‹ï¼šFlask + LINE Webhook + Google Sheets +ï¼ˆé¸ï¼‰Vision OCR
#
# æŒ‡ä»¤åªè™•ç†ä»¥ä¸‹ï¼š
#   #æˆ‘çš„IDã€#å¯„æ›¸ã€#æŸ¥å¯„æ›¸ã€#å–æ¶ˆå¯„æ›¸ã€#åˆªé™¤å¯„æ›¸ã€#åˆªé™¤å‡ºæ›¸ã€#å–æ¶ˆå‡ºæ›¸ã€#å‡ºæ›¸ã€#è²·æ›¸ã€#å…¥åº«
# å…¶ä»–æ–‡å­—ï¼åœ–ç‰‡ï¼šä¸€å¾‹ä¸è™•ç†ã€ä¸å›è¦†ã€‚
#
# æ¨¡çµ„ç´¢å¼•ï¼ˆå®Œæ•´ PY åŸå‰‡ï¼‰ï¼š
# - åŠŸèƒ½ Aï¼šç™½åå–®é©—è­‰ + å€™é¸åå–®è¨˜éŒ„
# - åŠŸèƒ½ Bï¼šå¯„æ›¸å»ºç«‹ï¼ˆ#å¯„æ›¸ï¼›åœ°å€è‡ªå‹•è£œ3ç¢¼éƒµéå€è™Ÿï¼›â˜…â˜…å¤šæœ¬æ›¸åŒIDå±•é–‹å¤šåˆ—ï¼‰
# - åŠŸèƒ½ Cï¼šæŸ¥å¯„æ›¸ï¼ˆé›»è©±å¾Œç¢¼æ¨¡ç³Šæ¯”å°ï¼›â˜…â˜…åŒä¸€IDåˆä½µé¡¯ç¤ºå¤šæœ¬ï¼‰
# - åŠŸèƒ½ Dï¼šå–æ¶ˆ/åˆªé™¤å¯„æ›¸ï¼ˆâ˜…â˜…åŒä¸€IDå…¨éƒ¨åˆªé™¤ï¼›éœ€å»ºå–®äººï¼›â€»æ’åºèˆ‡å·²åˆªé™¤ä¿®è£œï¼‰
# - åŠŸèƒ½ Eï¼šå‡ºæ›¸ OCR å•Ÿç”¨ï¼ˆ#å‡ºæ›¸ é–‹å•Ÿ10åˆ†é˜æœƒè©±ï¼Œæœªé–‹å•Ÿä¸å›è¦†åœ–ç‰‡ï¼‰
# - åŠŸèƒ½ Fï¼šOCR è§£æ + å¯«å›ï¼ˆR#### â†” 12ç¢¼å–®è™Ÿã€å¯„å‡ºæ—¥æœŸã€ç¶“æ‰‹äººã€ç‹€æ…‹ï¼‰
# - åŠŸèƒ½ Gï¼šåˆªé™¤/å–æ¶ˆå‡ºæ›¸ï¼ˆæ’¤éŠ·å·²è¨—é‹æ¬„ä½ï¼Œç‹€æ…‹å›å¾…è™•ç†ï¼‰
# - åŠŸèƒ½ Hï¼šå…¥åº«ï¼ˆ#è²·æ›¸/#å…¥åº«ï¼šæ›¸åè¾¨è­˜â†’OKç¢ºèªâ†’å¯«å…¥ï¼›æ”¯æ´è² æ•¸ï¼ç›¤é»èª¿æ•´ï¼‰
# - åŠŸèƒ½ Iï¼š#æˆ‘çš„IDï¼ˆä¸å—ç™½åå–®é™åˆ¶ï¼‰
# - åŠŸèƒ½ Jï¼šåªå›è¦†æŒ‡å®šæŒ‡ä»¤ï¼å…¶ä»–ä¸€å¾‹ä¸å›è¦†
# ============================================

from flask import Flask, request, abort
import os, re, io, json, difflib, logging, time
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

import gspread
from google.oauth2.service_account import Credentials

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import (
    MessageEvent, TextMessage, ImageMessage,
    TextSendMessage,
)

# ====== ï¼ˆOCRï¼šä½¿ç”¨é¡¯å¼æ†‘è­‰ï¼‰======
_HAS_VISION = False
_vision_client = None
try:
    from google.cloud import vision
    from google.oauth2 import service_account as gcp_service_account
    _HAS_VISION = True
except Exception:
    _HAS_VISION = False
# ==================================

app = Flask(__name__)
app.logger.setLevel(logging.INFO)

# ---- ç’°å¢ƒè®Šæ•¸ ----
SHEET_ID = os.getenv("SHEET_ID", "").strip()

MAIN_SHEET_NAME = os.getenv("MAIN_SHEET_NAME", "å¯„æ›¸ä»»å‹™")
BOOK_MASTER_SHEET_NAME = os.getenv("BOOK_MASTER_SHEET_NAME", "æ›¸ç›®ä¸»æª”")
ZIPREF_SHEET_NAME = os.getenv("ZIPREF_SHEET_NAME", "éƒµéå€è™Ÿåƒç…§è¡¨")
STOCK_IN_SHEET_NAME = os.getenv("STOCK_IN_SHEET_NAME", "å…¥åº«æ˜ç´°")
HISTORY_SHEET_NAME = os.getenv("HISTORY_SHEET_NAME", "æ­·å²ç´€éŒ„")

# === ç™½åå–®è¨­å®šï¼ˆåŠŸèƒ½ Aï¼‰===
WHITELIST_SHEET_NAME = os.getenv("WHITELIST_SHEET_NAME", "ç™½åå–®")
CANDIDATE_SHEET_NAME = os.getenv("CANDIDATE_SHEET_NAME", "å€™é¸åå–®")
# WHITELIST_MODE: off | log | enforce
WHITELIST_MODE = os.getenv("WHITELIST_MODE", "enforce").strip().lower()
ADMIN_USER_IDS = {x.strip() for x in os.getenv("ADMIN_USER_IDS", "").split(",") if x.strip()}
_WHITELIST_CACHE = {"ts": 0.0, "set": set()}
_WHITELIST_TTL = 300  # ç§’

FUZZY_THRESHOLD = float(os.getenv("FUZZY_THRESHOLD", "0.6"))
QUERY_DAYS = int(os.getenv("QUERY_DAYS", "30"))
PHONE_SUFFIX_MATCH = int(os.getenv("PHONE_SUFFIX_MATCH", "9"))
WRITE_ZIP_TO_ADDRESS = os.getenv("WRITE_ZIP_TO_ADDRESS", "true").lower() == "true"
LOG_OCR_RAW = os.getenv("LOG_OCR_RAW", "true").lower() == "true"
OCR_SESSION_TTL_MIN = int(os.getenv("OCR_SESSION_TTL_MIN", "10"))

LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "")

if not LINE_CHANNEL_SECRET or not LINE_CHANNEL_ACCESS_TOKEN:
    raise RuntimeError("Missing LINE credentials.")

line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(LINE_CHANNEL_SECRET)

TZ = ZoneInfo("Asia/Taipei")

# ============================================
# Google Sheets é€£ç·š + è¡¨é ­å°æ‡‰
# ============================================
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

def _build_gspread_client():
    json_path = "service_account.json"
    if os.path.exists(json_path):
        creds = Credentials.from_service_account_file(json_path, scopes=SCOPES)
        return gspread.authorize(creds)
    sa_json = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON", "")
    if not sa_json:
        raise RuntimeError("Missing service account credentials. Provide service_account.json OR env GOOGLE_SERVICE_ACCOUNT_JSON.")
    creds = Credentials.from_service_account_info(json.loads(sa_json), scopes=SCOPES)
    return gspread.authorize(creds)

gc = _build_gspread_client()
ss = gc.open_by_key(SHEET_ID)

def _ws(name: str):
    return ss.worksheet(name)

def _get_or_create_ws(name: str, headers: list[str]):
    """è‹¥å·¥ä½œè¡¨ä¸å­˜åœ¨å‰‡å»ºç«‹ï¼Œä¸¦è£œä¸Šè¡¨é ­"""
    try:
        ws = ss.worksheet(name)
    except gspread.WorksheetNotFound:
        ws = ss.add_worksheet(title=name, rows=200, cols=max(10, len(headers)))
        if headers:
            ws.update(f"A1:{chr(64+len(headers))}1", [headers])
    return ws

def _get_header_map(ws):
    header = ws.row_values(1)
    hmap = {}
    for idx, title in enumerate(header, start=1):
        t = str(title).strip()
        if t:
            hmap[t] = idx
    return hmap

def _col_idx(hmap, key, default_idx):
    return hmap.get(key, default_idx)

# ============================================
# åŠŸèƒ½ Aï¼šç™½åå–®ï¼ˆuser_id é©—è­‰ + å€™é¸åå–®è¨˜éŒ„ï¼‰
# ============================================
def _truthy(v) -> bool:
    s = str(v).strip().lower()
    return s in ("1","true","yes","y","t","å•Ÿç”¨","æ˜¯","enabled","on")

def _load_whitelist(force: bool = False) -> set[str]:
    """å›å‚³ enabled çš„ user_id setï¼Œå¿«å– 5 åˆ†é˜"""
    now = time.time()
    if (not force) and _WHITELIST_CACHE["set"] and (now - _WHITELIST_CACHE["ts"] < _WHITELIST_TTL):
        return _WHITELIST_CACHE["set"]
    ws = _get_or_create_ws(WHITELIST_SHEET_NAME, ["user_id","name","enabled"])
    rows = ws.get_all_records()
    enabled = {str(r.get("user_id","")).strip() for r in rows if str(r.get("user_id","")).strip() and _truthy(r.get("enabled", "1"))}
    _WHITELIST_CACHE["set"] = enabled
    _WHITELIST_CACHE["ts"] = now
    return enabled

def _log_candidate(user_id: str, name: str):
    """è‡ªå‹•è¨˜éŒ„åˆ°å€™é¸åå–®ï¼ˆè‹¥å·²å­˜åœ¨åªæ›´æ–° last_seenï¼‰"""
    try:
        ws = _get_or_create_ws(CANDIDATE_SHEET_NAME, ["user_id","name","first_seen","last_seen"])
        all_vals = ws.get_all_values()
        h = _get_header_map(ws)
        idx_uid = _col_idx(h, "user_id", 1)
        idx_name = _col_idx(h, "name", 2)
        idx_first = _col_idx(h, "first_seen", 3)
        idx_last = _col_idx(h, "last_seen", 4)

        exists_row = None
        for i, r in enumerate(all_vals[1:], start=2):
            if (len(r) >= idx_uid) and r[idx_uid-1] == user_id:
                exists_row = i
                break

        now_s = datetime.now(TZ).strftime("%Y-%m-%d %H:%M")
        if exists_row:
            if name:
                try:
                    ws.update_cell(exists_row, idx_name, name)
                except Exception:
                    pass
            ws.update_cell(exists_row, idx_last, now_s)
        else:
            ws.append_row([user_id, name, now_s, now_s], value_input_option="USER_ENTERED")
    except Exception as e:
        app.logger.info(f"[CANDIDATE] log failed: {e}")

def _ensure_authorized(event, scope: str = "*") -> bool:
    """æ ¹æ“š WHITELIST_MODE åˆ¤æ–·æ˜¯å¦æ”¾è¡Œï¼›æœªæˆæ¬Šæ™‚å›è¦†èªªæ˜ä¸¦é™„ä½¿ç”¨è€…ID"""
    try:
        uid = getattr(event.source, "user_id", "")
        profile = line_bot_api.get_profile(uid)
        display_name = profile.display_name or "LINEä½¿ç”¨è€…"
    except Exception:
        uid = getattr(event.source, "user_id", "")
        display_name = "LINEä½¿ç”¨è€…"

    # å€™é¸åå–®æ°¸é è¨˜éŒ„
    if uid:
        _log_candidate(uid, display_name)

    if uid in ADMIN_USER_IDS:
        return True
    if WHITELIST_MODE in ("off", "log"):
        return True  # å…è¨±

    allowed = _load_whitelist()
    if uid in allowed:
        return True

    # æœªæˆæ¬Š â†’ æ–‡å­—æ‰æç¤ºï¼›åœ–ç‰‡ä¸å›è¦†
    if scope == "text":
        msg = f"âŒ å°šæœªæˆæ¬Šä½¿ç”¨ã€‚\nè«‹å°‡æ­¤ ID æä¾›çµ¦ç®¡ç†å“¡é–‹é€šï¼š\n{uid}\n\nï¼ˆæç¤ºï¼šå‚³ã€Œ#æˆ‘çš„IDã€ä¹Ÿèƒ½å–å¾—é€™ä¸² IDï¼‰"
        try:
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text=msg))
        except Exception:
            pass
    return False

# ============================================
# å·¥å…·èˆ‡æ ¼å¼åŒ–
# ============================================
def now_str_min():
    return datetime.now(TZ).strftime("%Y-%m-%d %H:%M")

def today_str():
    return datetime.now(TZ).strftime("%Y-%m-%d")

def normalize_phone(s: str):
    digits = re.sub(r"\D+", "", s or "")
    if len(digits) == 10 and digits.startswith("09"):
        return digits
    return None

def parse_kv_lines(text: str):
    data = {}
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    for ln in lines:
        if ln.startswith("#"):
            continue
        if "ï¼š" in ln:
            k, v = ln.split("ï¼š", 1)
        elif ":" in ln:
            k, v = ln.split(":", 1)
        else:
            k, v = "_free_", ln
        k = k.strip()
        v = v.strip()
        data.setdefault(k, []).append(v)
    return data

def col_to_letter(col: int) -> str:
    s = ""
    while col > 0:
        col, r = divmod(col - 1, 26)
        s = chr(r + 65) + s
    return s

# ============================================
# åŠŸèƒ½ Xï¼šåŸå§‹è³‡æ–™ â†’ #å¯„æ›¸ æ ¼å¼åŒ–ï¼ˆfor #æ•´ç†å¯„æ›¸ï¼‰
# ============================================
def _parse_raw_to_order(text: str):
    """
    è¼¸å…¥ï¼šå¤šè¡ŒåŸå§‹è³‡æ–™ï¼ˆå§“å/é›»è©±/æ›¸å + åœ°å€ + å‚™è¨»ï¼‰
    è¼¸å‡ºï¼šdictï¼ˆname, phone, address, book_raw, biz_noteï¼‰
    """
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None, ["âŒ æ²’æœ‰è®€åˆ°ä»»ä½•å…§å®¹"]

    # é è¨­ï¼šç¬¬ä¸€è¡Œï¼ˆå§“å + é›»è©± + æ›¸åï¼‰
    first = lines[0]
    rest = lines[1:]

    # é›»è©±
    phone = None
    m = re.search(r"(09\d{8})", first)
    if m:
        phone = m.group(1)
        first = first.replace(phone, "").strip()

    # å§“å + æ›¸åï¼ˆé›»è©±å»æ‰å¾Œï¼‰
    tokens = first.split()
    name = tokens[0] if tokens else None
    book_raw = " ".join(tokens[1:]) if len(tokens) > 1 else None

    # åœ°å€ï¼ˆå–å‰©ä¸‹ç¬¬ä¸€è¡Œï¼‰
    address, notes = None, []
    if rest:
        address = rest[0].replace(" ", "")
        if len(rest) > 1:
            notes = rest[1:]

    return {
        "name": name,
        "phone": phone,
        "address": address,
        "book_raw": book_raw,
        "biz_note": " / ".join(notes)
    }, []

# ============================================
# å¯„é€æ–¹å¼åµæ¸¬ï¼ˆåªåµæ¸¬ä¾¿åˆ©å•†åº—ï¼›å…¶é¤˜äº¤ç”±å¾ŒçºŒè¦å‰‡ï¼‰
# ============================================
def detect_delivery_method(text: str):
    s = (text or "").lower().replace("â€”", "-").replace("ï¼", "/")
    if any(k in s for k in ["7-11","7/11","7ï¼11","7â€“11","711","å°ä¸ƒ"]): return "7-11"
    if "å…¨å®¶" in s or "family" in s: return "å…¨å®¶"
    if "èŠçˆ¾å¯Œ" in s or "hi-life" in s or "hilife" in s: return "èŠçˆ¾å¯Œ"
    if "ok" in s or "okè¶…å•†" in s: return "OK"
    return None

# ============================================
# éƒµéå€è™ŸæŸ¥æ‰¾ï¼ˆå‰ç½®ï¼‰
# ============================================
_zip_cache = None
def _load_zipref():
    global _zip_cache
    if _zip_cache is not None:
        return _zip_cache
    try:
        ws = _ws(ZIPREF_SHEET_NAME)
        rows = ws.get_all_values()
        header = rows[0] if rows else []
        zi, ai = None, None
        for i, name in enumerate(header):
            n = str(name).strip()
            if zi is None and re.search(r"éƒµéå€è™Ÿ|éƒµé|zip|ZIP", n, re.I): zi = i
            if ai is None and re.search(r"åœ°å€|è·¯|å€|é„‰|é®|æ‘|é‡Œ|æ®µ|å··|å¸‚|ç¸£", n): ai = i
        if zi is None or ai is None:
            zi, ai = 1, 0
        pairs = []
        for r in rows[1:]:
            try:
                prefix = (r[ai] if ai < len(r) else "").strip()
                z = (r[zi] if zi < len(r) else "").strip()
                if prefix and re.fullmatch(r"\d{3}(\d{2})?", z):
                    pairs.append((prefix, z))
            except Exception:
                continue
        pairs.sort(key=lambda x: len(x[0]), reverse=True)
        _zip_cache = pairs
        return _zip_cache
    except Exception as e:
        app.logger.info(f"[ZIPREF] Load failed: {e}")
        _zip_cache = []
        return _zip_cache

def lookup_zip(address: str):
    if not address:
        return None
    pairs = _load_zipref()  # [("ä¸­æ­£å€","100"), ...]
    a = address.strip()
    for prefix, z in pairs:
        if prefix in a:
            return z
    return None

# ============================================
# æ›¸åæ¯”å°ï¼ˆå…±ç”¨ï¼‰
# ============================================

def _normalize_for_match(s: str) -> str:
    """æ¯”å°ç”¨å­—ä¸²æ­£è¦åŒ–ï¼šå°å¯«ã€å»ç©ºç™½ã€å»æ¨™é»"""
    return re.sub(r"[\s\W_]+", "", (s or "").lower())

def _split_alias_field(field: str):
    """æŠŠã€Œæ¨¡ç³Šæ¯”å°æ›¸åã€æ¬„ä½ä¾å¸¸è¦‹åˆ†éš”ç¬¦æ‹†é–‹ï¼Œä¸¦åšæ­£è¦åŒ–"""
    if not field:
        return []
    parts = re.split(r"[ã€,;|ï¼/ ]+", field)
    return [{"raw": p.strip(), "norm": _normalize_for_match(p)} for p in parts if p.strip()]

BOOK_INDEX_CACHE = None

def build_book_index(ws_books):
    """å¾ã€ˆæ›¸ç›®ä¸»æª”ã€‰å»ºç«‹æ¯”å°ç´¢å¼•ã€‚"""
    data = ws_books.get_all_values()
    if not data:
        return []

    header = {name: idx for idx, name in enumerate(data[0])}
    def col(name, default=None): return header.get(name, default)

    col_title = col("æ›¸ç±åç¨±")
    col_alias = col("æ¨¡ç³Šæ¯”å°æ›¸å")
    col_on    = col("æ˜¯å¦å•Ÿç”¨")

    index = []
    for row in data[1:]:
        title = (row[col_title] if col_title is not None and col_title < len(row) else "").strip()
        if not title:
            continue
        if col_on is not None and row[col_on].strip() != "ä½¿ç”¨ä¸­":
            continue
        alias_field = row[col_alias] if col_alias is not None and col_alias < len(row) else ""
        aliases = _split_alias_field(alias_field) or _split_alias_field(title)
        # å…ˆé•·å†çŸ­ â†’ é¿å…ã€ŒTry Nã€è“‹æ‰ã€ŒTry N5ã€
        aliases.sort(key=lambda a: len(a["norm"]), reverse=True)
        index.append({"title": title, "aliases": aliases})
    return index

def get_book_index():
    global BOOK_INDEX_CACHE
    if BOOK_INDEX_CACHE is None:
        ws_books = _ws(BOOK_MASTER_SHEET_NAME)
        BOOK_INDEX_CACHE = build_book_index(ws_books)
    return BOOK_INDEX_CACHE

def resolve_book_name(user_input: str):
    """è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œå›å‚³ï¼š(æ­£å¼æ›¸å, æ¯”å°æ–¹å¼, å€™é¸æ¸…å–®)"""
    src_norm = _normalize_for_match(user_input)
    if not src_norm:
        return (None, "notfound", [])

    # ğŸ”‘ æŠ“å‡ºè¼¸å…¥è£¡çš„æ•¸å­—
    digits = re.findall(r"\d+", user_input)

    books = get_book_index()

    # 1) å®Œå…¨ç›¸ç­‰
    for b in books:
        for alias in b["aliases"]:
            if src_norm == alias["norm"]:
                return (b["title"], "exact", None)

    # 2) è‹¥è¼¸å…¥å«æ•¸å­— â†’ åƒ…å…è¨±åŒæ¨£æ•¸å­—çš„æ›¸é€²ä¾†
    narrowed_books = books
    if digits:
        narrowed_books = []
        for b in books:
            aliases_norm = [a["norm"] for a in b["aliases"]]
            if any(any(d == dd for dd in re.findall(r"\d+", a)) for d in digits for a in aliases_norm):
                narrowed_books.append(b)
        if not narrowed_books:
            return (None, "notfound", [])

    # 3) å®Œæ•´åŒ…å«ï¼ˆè‡³å°‘4ç¢¼ï¼‰
    for b in narrowed_books:
        for alias in b["aliases"]:
            if len(alias["norm"]) >= 4 and alias["norm"] in src_norm:
                return (b["title"], "contain", None)

    # 4) Fuzzy æ¯”å°ï¼ˆéæ¿¾éçŸ­ aliasï¼‰
    universe, reverse_map = [], {}
    for b in narrowed_books:
        for alias in b["aliases"]:
            norm = alias["norm"]
            if not norm:
                continue
            if len(norm) < 3 and not re.match(r"^[a-z]\d$", norm):
                continue
            universe.append(norm)
            reverse_map[norm] = b["title"]

    matches = difflib.get_close_matches(src_norm, universe, n=5, cutoff=0.8)
    if not matches:
        return (None, "notfound", [])

    formal = []
    for m in matches:
        fm = reverse_map.get(m)
        if fm and fm not in formal:
            formal.append(fm)

    if len(formal) == 1:
        return (formal[0], "fuzzy", None)

    return (None, "ambiguous", formal)


# ========== æŠ½æ›¸åï¼ˆæ•´å¥æœ€é•·å„ªå…ˆï¼‹æ•¸å­—ç›¸ç¬¦ï¼‰ ==========
def _extract_books_and_note_from_text(user_text: str):
    """
    å¾åŸå§‹ä¸€å¥è©±ä¸­ï¼Œæ‰¾å‡ºã€æ›¸åã€(å¯å¤šæœ¬) èˆ‡ã€å‚™è¨»ã€ã€‚
    è¦å‰‡ï¼š
      - å…ˆæŠŠæ‰€æœ‰æ›¸ç›®çš„åˆ¥åæ”¤å¹³æˆæ¸…å–®ï¼Œä¾ã€Œåˆ¥åé•·åº¦ã€ç”±é•·åˆ°çŸ­æª¢æŸ¥
      - è‹¥è¼¸å…¥ä¸­æœ‰æ•¸å­—ï¼Œåƒ…å…è¨±åˆ¥åä¹Ÿå«ã€ç›¸åŒæ•¸å­—ã€çš„æ›¸å‘½ä¸­ï¼ˆé¿å… LG1 èˆ‡ LG6 æ··æ·†ï¼‰
      - å‘½ä¸­å¾ŒæŠŠè©²åˆ¥åè‡ªåŸå¥ç§»é™¤ï¼Œå‰©é¤˜å³ç‚ºå‚™è¨»
    å›å‚³ï¼š(books: List[str], note: str)
    """
    raw = (user_text or "").strip()
    norm = _normalize_for_match(raw)
    if not norm:
        return [], raw

    # å–è¼¸å…¥ä¸­çš„æ•¸å­—ï¼ˆä¾‹å¦‚ "Lets Go 6 æ‰£é»" -> {"6"})
    digits_in_input = set(re.findall(r"\d+", raw))

    # æ”¤å¹³æ‰€æœ‰æ›¸çš„æ‰€æœ‰åˆ¥åï¼Œä¾é•·åº¦ç”±é•·åˆ°çŸ­
    flat_aliases = []
    for b in get_book_index():
        for a in b["aliases"]:
            flat_aliases.append((b["title"], a["raw"], a["norm"]))
    flat_aliases.sort(key=lambda t: len(t[2] or ""), reverse=True)

    matched_titles = []
    matched_alias_raws = []
    seen_titles = set()

    for title, alias_raw, alias_norm in flat_aliases:
        if title in seen_titles:
            continue
        if not alias_norm:
            continue

        # è‹¥è¼¸å…¥æœ‰æ•¸å­—ï¼Œè¦æ±‚åˆ¥åä¹Ÿå¸¶åˆ°ç›¸åŒæ•¸å­—ï¼ˆè‡³å°‘é‡ç–Šä¸€å€‹ï¼‰
        if digits_in_input:
            alias_digits = set(re.findall(r"\d+", alias_raw))
            if not alias_digits.intersection(digits_in_input):
                continue

        # åªç”¨ã€Œå®Œæ•´åŒ…å«ã€åˆ¤æ–·ï¼ˆalias_norm å¿…é ˆæ•´æ®µåœ¨è¼¸å…¥ norm ä¸­ï¼‰
        if alias_norm in norm:
            matched_titles.append(title)
            matched_alias_raws.append(alias_raw)
            seen_titles.add(title)

    # å¾åŸå¥ç§»é™¤å·²å‘½ä¸­çš„ã€å¯è®€åˆ¥åã€ï¼Œå‰©ä¸‹å³ç‚ºå‚™è¨»
    note = raw
    for araw in sorted(matched_alias_raws, key=len, reverse=True):
        note = re.sub(re.escape(araw), " ", note, flags=re.IGNORECASE)

    # ç§»é™¤é›»è©±èˆ‡å¸¸è¦‹å‚™è¨»è©
    note = re.sub(r"09\d{8}", " ", note)
    for w in ["æ‰£é»","è£œå¯„","é‡å¯„","æ”¹å¯„","æ”¹åœ°å€","è´ˆé€","è£œæ›¸","æ›æ›¸","é€€å›","æ€¥ä»¶","å‚™è¨»"]:
        note = note.replace(w, " ")
    note = re.sub(r"\s+", " ", note).strip()

    return matched_titles, note
# ======================================================


# ============================================
# Vision Clientï¼ˆé¡¯å¼æ†‘è­‰å»ºç«‹ï¼‰
# ============================================
def _build_vision_client():
    global _HAS_VISION, _vision_client
    if not _HAS_VISION:
        return None
    try:
        json_path = "service_account.json"
        creds = None
        if os.path.exists(json_path):
            creds = gcp_service_account.Credentials.from_service_account_file(json_path)
        else:
            sa_json = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON", "")
            if not sa_json:
                raise RuntimeError("Missing GOOGLE_SERVICE_ACCOUNT_JSON for Vision client.")
            creds = gcp_service_account.Credentials.from_service_account_info(json.loads(sa_json))
        _vision_client = vision.ImageAnnotatorClient(credentials=creds)
        return _vision_client
    except Exception as e:
        app.logger.info(f"[VISION] init failed: {e}")
        _HAS_VISION = False
        return None

_vision_client = _build_vision_client()

# ============================================
# å»ºæª”è¼”åŠ©ï¼ˆå¯„æ›¸ï¼‰
# ============================================
def _gen_next_record_id(ws, header_map):
    colA = _col_idx(header_map, "ç´€éŒ„ID", 1)
    values = ws.col_values(colA)[1:]
    max_no = 0
    for v in values:
        m = re.fullmatch(r"R(\d{4})", str(v).strip())
        if m:
            n = int(m.group(1))
            if n > max_no:
                max_no = n
    return f"R{max_no+1:04d}"

def _build_insert_row(ws, data, who_display_name, *, force_rid=None, force_book=None):
    """
    æ¬„ä½ï¼šAç´€éŒ„ID Bå»ºå–®æ—¥æœŸ Cå»ºå–®äºº Då­¸å“¡å§“å Eå­¸å“¡é›»è©± Få¯„é€åœ°å€
          Gæ›¸ç±åç¨± Hæ¥­å‹™å‚™è¨» Iå¯„é€æ–¹å¼ Jå¯„å‡ºæ—¥æœŸ Kè¨—é‹å–®è™Ÿ Lç¶“æ‰‹äºº Må¯„é€ç‹€æ…‹
    """
    hmap = _get_header_map(ws)
    header_len = len(ws.row_values(1))
    idxA = _col_idx(hmap, "ç´€éŒ„ID", 1)
    idxB = _col_idx(hmap, "å»ºå–®æ—¥æœŸ", 2)
    idxC = _col_idx(hmap, "å»ºå–®äºº", 3)
    idxD = _col_idx(hmap, "å­¸å“¡å§“å", 4)
    idxE = _col_idx(hmap, "å­¸å“¡é›»è©±", 5)
    idxF = _col_idx(hmap, "å¯„é€åœ°å€", 6)
    idxG = _col_idx(hmap, "æ›¸ç±åç¨±", 7)
    idxH = _col_idx(hmap, "æ¥­å‹™å‚™è¨»", 8)
    idxI = _col_idx(hmap, "å¯„é€æ–¹å¼", 9)
    idxJ = _col_idx(hmap, "å¯„å‡ºæ—¥æœŸ", 10)
    idxK = _col_idx(hmap, "è¨—é‹å–®è™Ÿ", 11)
    idxL = _col_idx(hmap, "ç¶“æ‰‹äºº", 12)
    idxM = _col_idx(hmap, "å¯„é€ç‹€æ…‹", 13)

    total_cols = max(header_len, idxM)
    row = [""] * total_cols

    if force_rid:
        rid = force_rid
    else:
        rid = _gen_next_record_id(ws, hmap)

    row[idxA-1] = rid
    row[idxB-1] = now_str_min()
    row[idxC-1] = who_display_name or "LINEä½¿ç”¨è€…"
    row[idxD-1] = data.get("name","")
    phone = data.get("phone","")
    row[idxE-1] = f"'{phone}" if phone else ""
    address = data.get("address","")

    # âœ… ä¸€å¾‹å˜—è©¦è£œéƒµéå€è™Ÿï¼ˆè‹¥èƒ½å°ä¸Šä¸”ç›®å‰æ²’3ç¢¼é–‹é ­ï¼‰
    if WRITE_ZIP_TO_ADDRESS and address:
        z = lookup_zip(address)
        if z and not re.match(r"^\d{3}", address):
            address = f"{z}{address}"
    row[idxF-1] = address

    # â˜…â˜… å…è¨±è¦†å¯«æ›¸åï¼ˆå¤šæœ¬å±•é–‹ï¼‰
    row[idxG-1] = force_book if force_book else data.get("book_formal","")
    row[idxH-1] = data.get("biz_note","")
    row[idxI-1] = data.get("delivery") or ""
    row[idxJ-1] = ""
    row[idxK-1] = ""
    row[idxL-1] = ""
    row[idxM-1] = "å¾…è™•ç†"

    return row, {"rid": rid}

# â˜… æ’å…¥ç¬¬2åˆ—ä¸ç¹¼æ‰¿æ ¼å¼
def _insert_row_values_no_inherit(ws, row_values, index=2):
    ss.batch_update({
        "requests": [{
            "insertDimension": {
                "range": {
                    "sheetId": ws.id,
                    "dimension": "ROWS",
                    "startIndex": index - 1,
                    "endIndex": index
                },
                "inheritFromBefore": False
            }
        }]
    })
    header_len = len(ws.row_values(1)) or 13
    last_col = max(header_len, len(row_values))
    if len(row_values) < last_col:
        row_values = row_values + [""] * (last_col - len(row_values))
    rng = f"A{index}:{col_to_letter(last_col)}{index}"
    ws.update(rng, [row_values], value_input_option="USER_ENTERED")

# ============================================
# åŠŸèƒ½ Xï¼šåŸå§‹è³‡æ–™ â†’ #å¯„æ›¸ æ ¼å¼åŒ–ï¼ˆfor #æ•´ç†å¯„æ›¸ï¼‰
# ä½¿ç”¨æ•´å¥æœ€é•·å„ªå…ˆæ¯”å° â†’ å…ˆæŠ“æ›¸åï¼Œå‰©ä¸‹è®Šå‚™è¨»
# ============================================
def _parse_raw_to_order(text: str):
    # å…ˆæŠŠå¼•è™Ÿ/å…¨å½¢ç©ºç™½è™•ç†ä¹¾æ·¨
    text = (text or "").replace("\u3000", " ").strip()
    text = text.strip('"').strip("'")
    lines = [ln.strip().strip('"').strip("'") for ln in text.splitlines() if ln.strip()]
    if not lines:
        return None, ["âŒ æ²’æœ‰è®€åˆ°ä»»ä½•å…§å®¹"]

    # é›»è©±ï¼ˆå…¨æ–‡æŠ“ 09 é–‹é ­ 10 ç¢¼ï¼‰
    joined = " ".join(lines)
    m_phone = re.search(r"(09\d{8})", joined)
    phone = m_phone.group(1) if m_phone else None
    if phone:
        lines = [ln.replace(phone, "").strip() for ln in lines]

    # åœ°å€ï¼šå¾ç¬¬2è¡Œé–‹å§‹æ‰¾å¸¸è¦‹åœ°å/è·¯å­—çœ¼
    addr_idx, address = None, None
    for i, ln in enumerate(lines[1:], start=1):
        if re.search(r"(å¸‚|ç¸£).*(å€|é„‰|é®|å¸‚)|è·¯|è¡—|æ®µ|å··|å¼„|è™Ÿ|æ¨“", ln):
            address, addr_idx = ln.replace(" ", ""), i
            break

    # å§“åï¼šç¬¬ä¸€è¡Œå‰æ®µçš„éæ•¸å­—é€£çºŒå­—å…ƒ
    first = lines[0]
    m_name = re.match(r"^[^\d\s]+", first)
    name = m_name.group(0) if m_name else None
    rest_first = first[len(name):].strip() if name else first

    # æº–å‚™ä¸€ä¸²ã€Œç”¨ä¾†æŠ½æ›¸åã€çš„æ–‡å­—ï¼ˆä¸è¦å†æ‹† tokenï¼‰
    other_lines = [ln for idx, ln in enumerate(lines[1:], start=1) if idx != addr_idx]
    candidate_text = " ".join([rest_first] + other_lines)

    # â˜… é€™è£¡ç”¨æ•´å¥æœ€é•·å„ªå…ˆå»æŠ½æ›¸åï¼Œå‰©é¤˜è®Šå‚™è¨»
    books, note = _extract_books_and_note_from_text(candidate_text)

    return {
        "name": name,
        "phone": phone,
        "address": address,
        "book_list": books,
        "book_raw": "ã€".join(books) if books else "",
        "biz_note": note
    }, []


# ============================================
# åŠŸèƒ½ Bï¼šè§£æï¼‹å»ºç«‹å¯„æ›¸ï¼ˆ#å¯„æ›¸ï¼‰
# ï¼ˆâ˜…â˜… å¤šæœ¬æ›¸æ”¯æ´ï¼šåŒä¸€IDå±•é–‹å¤šåˆ—ï¼›å¯„é€æ–¹å¼ä¹Ÿæª¢æŸ¥åœ°å€ï¼‰
# ============================================
_BOOK_SPLIT_RE = re.compile(r"[ã€ï¼Œ,ï¼/\s\t]+")

def _parse_new_order_text(raw_text: str):
    data = parse_kv_lines(raw_text)

    # 1) å§“å
    name = None
    for k in list(data.keys()):
        if any(x in k for x in ["å§“å","å­¸å“¡","æ”¶ä»¶äºº","åå­—","è²´å§“"]):
            name = "ã€".join(data.pop(k))
            break

    # 2) é›»è©±
    phone = None
    for k in list(data.keys()):
        if "é›»è©±" in k:
            for v in data.pop(k):
                p = normalize_phone(v)
                if p:
                    phone = p
                    break
            break

    # 3) åœ°å€
    address = None
    for k in list(data.keys()):
        if any(x in k for x in ["å¯„é€åœ°å€","åœ°å€","æ”¶ä»¶åœ°å€","é…é€åœ°å€"]):
            address = " ".join(data.pop(k))
            address = address.replace(" ", "")
            break

    # 4) æ›¸åï¼ˆå…ˆæ•´ä¸²ï¼‰
    book_raw = None
    for k in list(data.keys()):
        if any(x in k for x in ["æ›¸","æ›¸å","æ•™æ","æ›¸ç±åç¨±"]):
            book_raw = " ".join(data.pop(k)).strip()
            break

    # åˆä½µå‰©é¤˜æ–‡å­—ä»¥åˆ©åµæ¸¬ä¾¿åˆ©å•†åº—
    merged_text = "\n".join(sum(data.values(), []))

    # â˜…â˜… å¯„é€æ–¹å¼æª¢æ¸¬æ“´å¤§åˆ°ã€Œå‰©é¤˜æ–‡å­— + åœ°å€ã€
    delivery = detect_delivery_method(merged_text + " " + (address or ""))

    # è‹¥æœªåµæ¸¬åˆ°ä¾¿åˆ©å•†åº—ã€ä½†æœ‰åœ°å€ â†’ å¯„é€æ–¹å¼=ã€Œä¾¿åˆ©å¸¶ã€
    if not delivery and address:
        delivery = "ä¾¿åˆ©å¸¶"

    # å…¶ä»–æ–‡å­— â†’ æ¥­å‹™å‚™è¨»
    others = []
    for k, arr in data.items():
        for v in arr:
            if k != "_free_":
                others.append(f"{k}ï¼š{v}")
            else:
                others.append(v)
    biz_note = " / ".join([x for x in others if x.strip()])

    # é©—è­‰ï¼ˆè‹¥éä¾¿åˆ©å•†åº—éœ€åœ°å€ï¼‰
    errors = []
    if not name: errors.append("ç¼ºå°‘ã€å§“åã€‘")
    if not phone: errors.append("é›»è©±æ ¼å¼éŒ¯èª¤ï¼ˆéœ€ 09 é–‹é ­ 10 ç¢¼ï¼‰")
    if not book_raw: errors.append("ç¼ºå°‘ã€æ›¸åã€‘")
    if delivery not in ["7-11","å…¨å®¶","OK","èŠçˆ¾å¯Œ"] and not address:
        errors.append("ç¼ºå°‘ã€å¯„é€åœ°å€ã€‘ï¼ˆéè¶…å•†å¿…å¡«ï¼‰")

    return {
        "name": name,
        "phone": phone,
        "address": address,
        "book_raw": book_raw,
        "biz_note": biz_note,
        "delivery": delivery,
        "raw_text": raw_text
    }, errors

def _handleæ•´ç†å¯„æ›¸(event, text):
    body = re.sub(r"^#æ•´ç†å¯„æ›¸\s*", "", text.strip())
    parsed, errs = _parse_raw_to_order(body)
    if errs:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="\n".join(errs)))
        return

    books_line = parsed.get("book_raw") or ""
    warn = "\n\nâš ï¸ æœªè¾¨è­˜åˆ°æ›¸åï¼Œè«‹è£œå……æˆ–èª¿æ•´å¾Œå†ç¢ºèªã€‚" if not books_line else ""

    msg = (
        "#å¯„æ›¸\n"
        f"å§“åï¼š{parsed.get('name') or ''}\n"
        f"é›»è©±ï¼š{parsed.get('phone') or ''}\n"
        f"å¯„é€åœ°å€ï¼š{parsed.get('address') or ''}\n"
        f"æ›¸ç±åç¨±ï¼š{books_line}\n"
        f"å‚™è¨»ï¼š{parsed.get('biz_note') or ''}"
        f"{warn}"
    )

    _PENDING[event.source.user_id] = {
        "type": "æ•´ç†å¯„æ›¸_confirm",
        "data": parsed,
    }
    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(text="è«‹ç¢ºèªä»¥ä¸‹è³‡è¨Šï¼š\n\n" + msg + "\n\nå›è¦† OK / YES ç¢ºèªï¼›å›è¦† N å–æ¶ˆã€‚")
    )


def _handle_new_order(event, text):
    try:
        profile = line_bot_api.get_profile(event.source.user_id)
        display_name = profile.display_name
    except Exception:
        display_name = "LINEä½¿ç”¨è€…"

    parsed, errs = _parse_new_order_text(text)
    if errs:
        msg = "âŒ å»ºæª”å¤±æ•—ï¼š\n- " + "\n- ".join(errs)
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=msg))
        return

    # â˜…â˜… å¤šæœ¬æ›¸åˆ‡å‰² + å€‹åˆ¥è§£æ
    raw_list = [s for s in _BOOK_SPLIT_RE.split(parsed["book_raw"] or "") if s.strip()]
    if not raw_list:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ å»ºæª”å¤±æ•—ï¼šæœªè®€åˆ°æœ‰æ•ˆæ›¸å"))
        return

    formal_list = []
    amb_msgs, nf_msgs = [], []
    for token in raw_list:
        bk, kind, extra = resolve_book_name(token)
        if not bk:
            if kind == "ambiguous" and extra:
                amb_msgs.append(f"ã€Œ{token}ã€å¯èƒ½æ˜¯ï¼š{ 'ã€'.join(extra[:10]) }")
            else:
                nf_msgs.append(f"æ‰¾ä¸åˆ°æ›¸åï¼š{token}")
        else:
            formal_list.append(bk)

    if amb_msgs:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="â— æ›¸åä¸å¤ æ˜ç¢ºï¼š\n" + "\n".join(amb_msgs)))
        return
    if nf_msgs:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ æ‰¾ä¸åˆ°æ›¸åï¼š\n" + "\n".join(nf_msgs)))
        return

    # å–å¾—è¡¨ï¼ç”¢ç”ŸåŒä¸€çµ„ RID
    ws = _ws(MAIN_SHEET_NAME)
    rid = _gen_next_record_id(ws, _get_header_map(ws))

    # é€æœ¬æ›¸å±•é–‹ç‚ºå¤šåˆ—ï¼ˆåŒä¸€ RIDï¼‰
    # ç‚ºç¶­æŒè¼¸å…¥é †åºï¼Œé€£çºŒæ’å…¥ç¬¬2åˆ—æœƒé€ æˆååºï¼›å› æ­¤å€’åºæ’å…¥å³å¯ä¿æŒæœ€çµ‚è‡ªä¸Šè€Œä¸‹=åŸé †åºã€‚
    for bk in reversed(formal_list):
        row, _ = _build_insert_row(ws, parsed, display_name, force_rid=rid, force_book=bk)
        _insert_row_values_no_inherit(ws, row, index=2)

    resp = (
        "âœ… å·²æˆåŠŸå»ºæª”\n"
        f"ç´€éŒ„IDï¼š{rid}\n"
        f"å»ºå–®æ—¥æœŸï¼š{now_str_min()}\n"
        f"å§“åï¼š{parsed['name']}ï½œé›»è©±ï¼š{parsed['phone']}\n"
        f"åœ°å€ï¼š{row[_get_header_map(ws).get('å¯„é€åœ°å€',6)-1]}\n"
        f"æ›¸ç±ï¼š{'ã€'.join(formal_list)}\n"
        f"ç‹€æ…‹ï¼šå¾…è™•ç†"
    )
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=resp))

# ============================================
# åŠŸèƒ½ X-2ï¼š#æ•´ç†å¯„æ›¸ï¼ˆåŸå§‹è³‡æ–™ â†’ æ¨™æº– #å¯„æ›¸ æ ¼å¼ â†’ ç­‰å¾…ç¢ºèªï¼‰
# ============================================
def _handleæ•´ç†å¯„æ›¸(event, text):
    body = re.sub(r"^#æ•´ç†å¯„æ›¸\s*", "", text.strip())
    parsed, errs = _parse_raw_to_order(body)
    if errs:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="\n".join(errs)))
        return

    # çµ„æˆæ¨™æº–æ ¼å¼
    msg = (
        "#å¯„æ›¸\n"
        f"å§“åï¼š{parsed['name']}\n"
        f"é›»è©±ï¼š{parsed['phone']}\n"
        f"å¯„é€åœ°å€ï¼š{parsed['address']}\n"
        f"æ›¸ç±åç¨±ï¼š{parsed['book_raw']}\n"
        f"å‚™è¨»ï¼š{parsed['biz_note']}"
    )

    # å­˜å…¥ pendingï¼Œç­‰å¾…ä½¿ç”¨è€…å›è¦† OK
    _PENDING[event.source.user_id] = {
        "type": "æ•´ç†å¯„æ›¸_confirm",
        "data": parsed,
    }
    line_bot_api.reply_message(
        event.reply_token,
        TextSendMessage(
            text="è«‹ç¢ºèªä»¥ä¸‹è³‡è¨Šï¼š\n\n"
                 + msg +
                 "\n\nå›è¦† OK / YES ç¢ºèªï¼›å›è¦† N å–æ¶ˆã€‚"
        )
    )


# ============================================
# åŠŸèƒ½ Cï¼šæŸ¥è©¢å¯„æ›¸ï¼ˆé è¨­ä¸é¡¯ç¤ºã€Œå·²åˆªé™¤ã€ï¼‰
# ï¼ˆâ˜…â˜… åŒä¸€IDåˆä½µå¤šæœ¬é¡¯ç¤ºï¼‰
# ============================================
def _handle_query(event, text):
    q = re.sub(r"^#(æŸ¥è©¢å¯„æ›¸|æŸ¥å¯„æ›¸)\s*", "", text.strip())

    ws = _ws(MAIN_SHEET_NAME)
    h = _get_header_map(ws)
    idxA = _col_idx(h, "ç´€éŒ„ID", 1)
    idxB = _col_idx(h, "å»ºå–®æ—¥æœŸ", 2)
    idxD = _col_idx(h, "å­¸å“¡å§“å", 4)
    idxE = _col_idx(h, "å­¸å“¡é›»è©±", 5)
    idxG = _col_idx(h, "æ›¸ç±åç¨±", 7)
    idxI = _col_idx(h, "å¯„é€æ–¹å¼", 9)
    idxJ = _col_idx(h, "å¯„å‡ºæ—¥æœŸ", 10)
    idxK = _col_idx(h, "è¨—é‹å–®è™Ÿ", 11)
    idxM = _col_idx(h, "å¯„é€ç‹€æ…‹", 13)

    rows = ws.get_all_values()[1:]
    since = datetime.now(TZ) - timedelta(days=QUERY_DAYS)

    phone_digits = re.sub(r"\D+","", q)
    is_phone = len(phone_digits) >= 7

    # å…ˆç¯©è¿‘ 30 å¤©ä¸”éå·²åˆªé™¤ï¼›ä¾æŸ¥è©¢æ¢ä»¶éæ¿¾
    filtered = []
    for r in rows:
        try:
            st = (r[idxM-1] or "").strip()
            if st == "å·²åˆªé™¤":
                continue
            dt_str = r[idxB-1].strip()
            dt = None
            if dt_str:
                try:
                    dt = datetime.strptime(dt_str[:16], "%Y-%m-%d %H:%M").replace(tzinfo=TZ)
                except Exception:
                    dt = None
            if dt and dt < since:
                continue

            if is_phone:
                cand = re.sub(r"\D+","", r[idxE-1])
                if len(cand) >= PHONE_SUFFIX_MATCH and phone_digits[-PHONE_SUFFIX_MATCH:] == cand[-PHONE_SUFFIX_MATCH:]:
                    filtered.append(r)
            else:
                if q and q in r[idxD-1]:
                    filtered.append(r)
        except Exception:
            continue

    if not filtered:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ æŸ¥ç„¡è¿‘ 30 å¤©å…§çš„å¯„æ›¸ç´€éŒ„ï¼Œè«‹ç¢ºèªå§“åæˆ–é›»è©±æ˜¯å¦æ­£ç¢ºã€‚"))
        return

    # â˜…â˜… ä¾ RID åˆä½µ
    groups = {}
    for r in filtered:
        rid = (r[idxA-1] or "").strip()
        if not rid:
            continue
        g = groups.setdefault(rid, {
            "dt": r[idxB-1],
            "name": r[idxD-1],
            "books": [],
            "status_any": set(),
            "ship_out": "",
            "ship_via": "",
            "ship_no": ""
        })
        # ä»¥æœ€å¤§æ™‚é–“ç‚ºè©²ç¾¤ä¸»æ™‚é–“
        if r[idxB-1] > g["dt"]:
            g["dt"] = r[idxB-1]
        g["books"].append((r[idxG-1] or "").strip())
        st = (r[idxM-1] or "").strip()
        if st:
            g["status_any"].add(st)
        # å–è¼ƒæ–°çš„å‡ºè²¨æ¬„ä½
        j, i, k = (r[idxJ-1] or "").strip(), (r[idxI-1] or "").strip(), (r[idxK-1] or "").strip()
        if j or i or k:
            # ç²—ç•¥ç­–ç•¥ï¼šè‹¥å°šæœªæœ‰è³‡æ–™ï¼Œæˆ–é€™åˆ—æ™‚é–“è¼ƒæ–°ï¼Œå°±è¦†è“‹
            if not (g["ship_out"] or g["ship_via"] or g["ship_no"]) or r[idxB-1] >= g["dt"]:
                g["ship_out"], g["ship_via"], g["ship_no"] = j, i, k

    # ä¾å»ºå–®æ™‚é–“å€’åºï¼›å–å‰ 5 çµ„
    ordered = sorted(groups.items(), key=lambda kv: kv[1]["dt"], reverse=True)[:5]

    blocks = []
    for rid, g in ordered:
        books = "ã€".join(sorted(set([b for b in g["books"] if b])))
        name = g["name"]
        statuses = g["status_any"]
        if "å·²è¨—é‹" in statuses:
            lines = [f"ğŸ“¦ {name}ï¼ˆ{rid}ï¼‰ï¼š{books}"]
            if g["ship_out"]: lines.append(f"å·²æ–¼ {g['ship_out']}")
            if g["ship_via"]: lines.append(f"ç”± {g['ship_via']} å¯„å‡º")
            if g["ship_no"]:  lines.append(f"è¨—é‹å–®è™Ÿï¼š{g['ship_no']}")
            blocks.append("\n".join(lines))
        else:
            # å–ä¸€å€‹ä»£è¡¨ç‹€æ…‹ï¼ˆæ²’æœ‰å°±é¡¯ç¤ºå¾…è™•ç†ï¼‰
            st = next(iter(statuses)) if statuses else "å¾…è™•ç†"
            blocks.append(f"ğŸ“¦ {name}ï¼ˆ{rid}ï¼‰ï¼š{books} {st}")

    msg = "\n\n".join(blocks)
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=msg))

# ============================================
# åŠŸèƒ½ Dï¼šå–æ¶ˆå¯„æ›¸ï¼ˆâ˜…â˜…åŒä¸€IDå…¨éƒ¨åˆªé™¤ï¼›ç„¡åˆªé™¤ç·šï¼›å·²åˆªé™¤æ’é™¤ï¼‹æ­£ç¢ºæ’åº ä¿®è£œï¼‰
# æŒ‡ä»¤ï¼š#å–æ¶ˆå¯„æ›¸ / #åˆªé™¤å¯„æ›¸  +ï¼ˆå§“å/é›»è©±ï¼‰
# æ¬Šé™ï¼šå»ºå–®äººï¼ˆCæ¬„ï¼‰é ˆç­‰æ–¼æ“ä½œè€…çš„ LINE é¡¯ç¤ºåç¨±
# ç¢ºèªï¼šå›è¦† Y/N
# ============================================
_PENDING = {}  # user_id -> pending dict
_OCR_SESSION = {}  # user_id -> {"type":"ship","expire_ts": epoch}

def _extract_cancel_target(text: str):
    body = re.sub(r"^#(å–æ¶ˆå¯„æ›¸|åˆªé™¤å¯„æ›¸)\s*", "", text.strip())
    name, phone = None, None

    data = parse_kv_lines(body)
    for k in list(data.keys()):
        if any(x in k for x in ["å§“å","å­¸å“¡","æ”¶ä»¶äºº","åå­—","è²´å§“"]):
            name = "ã€".join(data.pop(k)); break
    for k in list(data.keys()):
        if "é›»è©±" in k:
            for v in data.pop(k):
                p = normalize_phone(v)
                if p: phone = p; break
            break

    if not name or not phone:
        tokens = re.split(r"\s+", body)
        for t in tokens:
            tt = t.strip()
            if not tt: continue
            p = normalize_phone(tt)
            if (not phone) and p: phone = p; continue
            if not name and not re.search(r"\d", tt): name = tt
    return (name, phone)

# ğŸ”§ ä¿®è£œç‰ˆï¼šæ’é™¤ã€Œå·²åˆªé™¤ã€ï¼Œç”¨å¯è§£æçš„å»ºå–®æ™‚é–“åšæ’åºï¼ˆå–æœ€è¿‘ä¸€ç­†ï¼‰
def _find_latest_order(ws, name, phone):
    h = _get_header_map(ws)
    idxA = _col_idx(h, "ç´€éŒ„ID", 1)
    idxB = _col_idx(h, "å»ºå–®æ—¥æœŸ", 2)
    idxC = _col_idx(h, "å»ºå–®äºº", 3)
    idxD = _col_idx(h, "å­¸å“¡å§“å", 4)
    idxE = _col_idx(h, "å­¸å“¡é›»è©±", 5)
    idxM = _col_idx(h, "å¯„é€ç‹€æ…‹", 13)

    all_vals = ws.get_all_values()
    rows = all_vals[1:]
    since = datetime.now(TZ) - timedelta(days=QUERY_DAYS)

    phone_suffix = None
    if phone:
        pd = re.sub(r"\D+","", phone)
        if len(pd) >= PHONE_SUFFIX_MATCH:
            phone_suffix = pd[-PHONE_SUFFIX_MATCH:]

    candidates = []
    for ridx, r in enumerate(rows, start=2):
        try:
            # 1) æ’é™¤ã€Œå·²åˆªé™¤ã€
            if (r[idxM-1] or "").strip() == "å·²åˆªé™¤":
                continue

            # 2) è§£æå»ºå–®æ—¥æœŸï¼Œè¶…éæŸ¥è©¢çª—ç¯„åœå‰‡è·³é
            dt_str = (r[idxB-1] or "").strip()
            dt = None
            if dt_str:
                try:
                    dt = datetime.strptime(dt_str[:16], "%Y-%m-%d %H:%M").replace(tzinfo=TZ)
                except Exception:
                    dt = None
            if dt and dt < since:
                continue

            # 3) æ¢ä»¶æ¯”å°ï¼ˆå§“ååŒ…å«ï¼›é›»è©±å¾Œ N ç¢¼ï¼‰
            ok = True
            if name and name not in (r[idxD-1] or ""):
                ok = False
            if phone_suffix:
                cand = re.sub(r"\D+","", r[idxE-1] or "")
                if not (len(cand) >= PHONE_SUFFIX_MATCH and cand[-PHONE_SUFFIX_MATCH:] == phone_suffix):
                    ok = False

            if ok:
                # ç”¨ã€Œå¯æ¯”è¼ƒçš„æ™‚é–“ã€ç•¶æ’åº keyï¼›è‹¥ç„¡æ³•è§£ææ™‚é–“ï¼Œç”¨ datetime.min å¢Šåº•
                key_dt = dt or datetime.min.replace(tzinfo=TZ)
                candidates.append((key_dt, ridx, r))
        except Exception:
            continue

    if not candidates:
        return (None, None)
    # å–å»ºå–®æ™‚é–“æœ€æ–°çš„ä¸€ç­†
    candidates.sort(key=lambda x: x[0], reverse=True)
    _, row_i, row = candidates[0]
    return row_i, row

def _collect_rows_by_rid(ws, rid: str):
    """å›å‚³è©² RID çš„æ‰€æœ‰ (row_index, row_values)"""
    h = _get_header_map(ws)
    idxA = _col_idx(h, "ç´€éŒ„ID", 1)
    all_vals = ws.get_all_values()[1:]
    out = []
    for ridx, r in enumerate(all_vals, start=2):
        try:
            if (r[idxA-1] or "").strip() == rid:
                out.append((ridx, r))
        except Exception:
            continue
    return out

def _handle_cancel_request(event, text):
    try:
        profile = line_bot_api.get_profile(event.source.user_id)
        display_name = profile.display_name or "LINEä½¿ç”¨è€…"
    except Exception:
        display_name = "LINEä½¿ç”¨è€…"

    name, phone = _extract_cancel_target(text)
    ws = _ws(MAIN_SHEET_NAME)
    h = _get_header_map(ws)
    idxA = _col_idx(h, "ç´€éŒ„ID", 1)
    idxB = _col_idx(h, "å»ºå–®æ—¥æœŸ", 2)
    idxC = _col_idx(h, "å»ºå–®äºº", 3)
    idxD = _col_idx(h, "å­¸å“¡å§“å", 4)
    idxG = _col_idx(h, "æ›¸ç±åç¨±", 7)
    idxJ = _col_idx(h, "å¯„å‡ºæ—¥æœŸ", 10)
    idxK = _col_idx(h, "è¨—é‹å–®è™Ÿ", 11)
    idxL = _col_idx(h, "ç¶“æ‰‹äºº", 12)
    idxM = _col_idx(h, "å¯„é€ç‹€æ…‹", 13)
    idxH = _col_idx(h, "æ¥­å‹™å‚™è¨»", 8)

    row_i, r = _find_latest_order(ws, name, phone)
    if not row_i:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ æ‰¾ä¸åˆ°ç´€éŒ„"))
        return

    creator = (r[idxC-1] or "").strip() or "LINEä½¿ç”¨è€…"
    if creator != display_name:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ ä½ æ²’æœ‰åˆªé™¤æ¬Šé™ï¼ˆè«‹è¯ç¹«ç®¡ç†è€…ï¼‰"))
        return

    rid = (r[idxA-1] or "").strip()
    all_rows = _collect_rows_by_rid(ws, rid)

    # â˜…â˜… è‹¥ä»»ä¸€åˆ—ç‚ºå·²è¨—é‹æˆ–æœ‰å‡ºæ›¸æ¬„ä½ â†’ ç¦æ­¢åˆªé™¤
    for _, rr in all_rows:
        status = (rr[idxM-1] or "").strip()
        if status == "å·²è¨—é‹":
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ å·²è¨—é‹ï¼Œç„¡æ³•åˆªé™¤"))
            return
        if (rr[idxJ-1] or rr[idxK-1]) and status != "å·²è¨—é‹":
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text="â— ç„¡æ³•è™•ç†ï¼Œè«‹ç§è¨Šå®¢æœã€‚"))
            return

    stu = (r[idxD-1] or "").strip()
    books = "ã€".join([rr[idxG-1] for _, rr in all_rows if (rr[idxG-1] or "").strip()])
    _PENDING[event.source.user_id] = {
        "type": "cancel_order",
        "sheet": MAIN_SHEET_NAME,
        "rid": rid,
        "rows": [ri for ri, _ in all_rows],
        "stu": stu,
        "book_list": books,
        "operator": display_name,
        "idx": {"H": idxH, "L": idxL, "M": idxM}
    }
    prompt = f"è«‹ç¢ºèªæ˜¯å¦åˆªé™¤æ•´ç­†å¯„æ›¸ï¼ˆåŒä¸€ID {rid} å…± {len(all_rows)} åˆ—ï¼‰ï¼š\nå­¸å“¡ï¼š{stu}\næ›¸åï¼š{books}\n[Y/N]"
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=prompt))

# ============================================
# åŠŸèƒ½ Gï¼šåˆªé™¤ï¼å–æ¶ˆã€Œå‡ºæ›¸ã€ï¼ˆæ’¤éŠ·å·²è¨—é‹æ¬„ä½ï¼‰
# æŒ‡ä»¤ï¼š#åˆªé™¤å‡ºæ›¸ / #å–æ¶ˆå‡ºæ›¸ +ï¼ˆå§“å/é›»è©±ï¼‰
# å‹•ä½œï¼šæ¸…ç©º å¯„å‡ºæ—¥æœŸ/è¨—é‹å–®è™Ÿ/ç¶“æ‰‹äººï¼Œç‹€æ…‹æ”¹ç‚ºã€Œå¾…è™•ç†ã€ï¼Œå‚™è¨»é™„ä¸Šæ™‚é–“æˆ³
# ============================================
def _extract_ship_delete_target(text: str):
    body = re.sub(r"^#(åˆªé™¤å‡ºæ›¸|å–æ¶ˆå‡ºæ›¸)\s*", "", text.strip())
    name, phone = None, None

    data = parse_kv_lines(body)
    for k in list(data.keys()):
        if any(x in k for x in ["å§“å","å­¸å“¡","æ”¶ä»¶äºº","åå­—","è²´å§“"]):
            name = "ã€".join(data.pop(k)); break
    for k in list(data.keys()):
        if "é›»è©±" in k:
            for v in data.pop(k):
                p = normalize_phone(v)
                if p: phone = p; break
            break

    if not name or not phone:
        tokens = re.split(r"\s+", body)
        for t in tokens:
            tt = t.strip()
            if not tt: continue
            p = normalize_phone(tt)
            if (not phone) and p: phone = p; continue
            if not name and not re.search(r"\d", tt): name = tt
    return (name, phone)

def _handle_delete_ship(event, text):
    try:
        profile = line_bot_api.get_profile(event.source.user_id)
        operator = profile.display_name or "LINEä½¿ç”¨è€…"
    except Exception:
        operator = "LINEä½¿ç”¨è€…"

    name, phone = _extract_ship_delete_target(text)
    ws = _ws(MAIN_SHEET_NAME)
    h = _get_header_map(ws)
    idxB = _col_idx(h, "å»ºå–®æ—¥æœŸ", 2)
    idxD = _col_idx(h, "å­¸å“¡å§“å", 4)
    idxE = _col_idx(h, "å­¸å“¡é›»è©±", 5)
    idxH = _col_idx(h, "æ¥­å‹™å‚™è¨»", 8)
    idxJ = _col_idx(h, "å¯„å‡ºæ—¥æœŸ", 10)
    idxK = _col_idx(h, "è¨—é‹å–®è™Ÿ", 11)
    idxL = _col_idx(h, "ç¶“æ‰‹äºº", 12)
    idxM = _col_idx(h, "å¯„é€ç‹€æ…‹", 13)

    rows = ws.get_all_values()[1:]
    since = datetime.now(TZ) - timedelta(days=QUERY_DAYS)

    phone_suffix = None
    if phone:
        pd = re.sub(r"\D+","", phone)
        if len(pd) >= PHONE_SUFFIX_MATCH:
            phone_suffix = pd[-PHONE_SUFFIX_MATCH:]

    candidates = []
    for ridx, r in enumerate(rows, start=2):
        try:
            dt_str = (r[idxB-1] or "").strip()
            dt = None
            if dt_str:
                try:
                    dt = datetime.strptime(dt_str[:16], "%Y-%m-%d %H:%M").replace(tzinfo=TZ)
                except Exception:
                    dt = None
            if dt and dt < since: continue

            ok = True
            if name and name not in (r[idxD-1] or ""): ok = False
            if phone_suffix:
                cand = re.sub(r"\D+","", r[idxE-1] or "")
                if not (len(cand) >= PHONE_SUFFIX_MATCH and cand[-PHONE_SUFFIX_MATCH:] == phone_suffix):
                    ok = False
            # åƒ…é–å®šã€Œå·²è¨—é‹ã€æˆ–å…·å‡ºæ›¸æ¬„ä½è€…
            shipped = ((r[idxM-1] or "").strip() == "å·²è¨—é‹") or (r[idxJ-1] or r[idxK-1])
            if ok and shipped:
                candidates.append((ridx, r))
        except Exception:
            continue

    if not candidates:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ æ‰¾ä¸åˆ°å¯æ’¤éŠ·çš„å‡ºæ›¸ç´€éŒ„ï¼ˆè¿‘30å¤©ï¼‰"))
        return

    # å–æœ€è¿‘ä¸€ç­†
    row_i, r = sorted(candidates, key=lambda x: x[1][idxB-1], reverse=True)[0]
    # æ¸…ç©ºæ¬„ä½ï¼Œç‹€æ…‹å›ã€Œå¾…è™•ç†ã€
    note = f"[æ’¤éŠ·å‡ºæ›¸ {now_str_min()}]"
    try:
        curr_h = ws.cell(row_i, idxH).value or ""
    except Exception:
        curr_h = ""
    ws.update_cell(row_i, idxH, (curr_h + " " + note).strip() if curr_h else note)
    ws.update_cell(row_i, idxJ, "")
    ws.update_cell(row_i, idxK, "")
    ws.update_cell(row_i, idxL, operator)
    ws.update_cell(row_i, idxM, "å¾…è™•ç†")

    line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âœ… å·²æ’¤éŠ·æœ€è¿‘ä¸€ç­†å‡ºæ›¸ï¼šæ¬„ä½å·²æ¸…ç©ºä¸¦æ¢å¾©ç‚ºå¾…è™•ç†"))

# ============================================
# åŠŸèƒ½ Eï¼šå‡ºæ›¸ OCR å•Ÿç”¨ï¼ˆ#å‡ºæ›¸ é–‹å•Ÿæœƒè©±ï¼‰
# ============================================
def _start_ocr_session(user_id: str):
    _OCR_SESSION[user_id] = {
        "type": "ship",
        "expire_ts": time.time() + OCR_SESSION_TTL_MIN * 60
    }

def _has_ocr_session(user_id: str) -> bool:
    info = _OCR_SESSION.get(user_id)
    if not info: return False
    if time.time() > info["expire_ts"]:
        _OCR_SESSION.pop(user_id, None)
        return False
    return True

def _clear_ocr_session(user_id: str):
    _OCR_SESSION.pop(user_id, None)

# ============================================
# åŠŸèƒ½ Fï¼šOCR è§£æ + å¯«å›è¨—é‹å–®è³‡è¨Šï¼ˆåŠ å¼·ç‰ˆé™¤éŒ¯ï¼‰
# ============================================
def _download_line_image_bytes(message_id: str) -> bytes:
    content = line_bot_api.get_message_content(message_id)
    return b"".join(chunk for chunk in content.iter_content())

def _ocr_text_from_bytes(img_bytes: bytes) -> str:
    if not _vision_client:
        raise RuntimeError("Vision ç”¨æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼ˆè«‹ç¢ºèª GOOGLE_SERVICE_ACCOUNT_JSON å·²è¨­å®šï¼Œä¸”å°ˆæ¡ˆå·²å•Ÿç”¨ Vision APIï¼‰ã€‚")
    image = vision.Image(content=img_bytes)
    resp = _vision_client.text_detection(image=image)
    if resp.error.message:
        raise RuntimeError(resp.error.message)
    text = resp.full_text_annotation.text if resp.full_text_annotation else ""
    return text or ""

def _pair_ids_with_numbers(text: str):
    if not text:
        return [], ["æœªè®€å–åˆ°æ–‡å­—"]
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if LOG_OCR_RAW:
        app.logger.info(f"[OCR_RAW_OUTPUT] {repr(text[:1000])}")

    rids, nums = [], []
    for i, ln in enumerate(lines):
        for m in re.finditer(r"R\d{4}", ln):
            rids.append((m.group(), i))
        for m in re.finditer(r"\d{12}", ln):
            nums.append((m.group(), i))

    pairs, used_num, leftovers = [], set(), []
    for rid, li in rids:
        chosen, best_dist = None, 999
        for no, lj in nums:
            if (no, lj) in used_num:
                continue
            d = abs(lj - li)
            if d < best_dist:
                best_dist = d
                chosen = (no, lj)
        if chosen:
            pairs.append((rid, chosen[0]))
            used_num.add(chosen)
        else:
            leftovers.append(f"{rid}ï½œæœªæ‰¾åˆ° 12 ç¢¼å–®è™Ÿ")

    for no, lj in nums:
        if (no, lj) not in used_num:
            leftovers.append(f"æœªé…å°å–®è™Ÿï¼š{no}")

    return pairs, leftovers

# â˜…â˜… åŠ å¼·ç‰ˆ OCR å¯«å›å‡½æ•¸ï¼ˆè©³ç´°é™¤éŒ¯æ—¥èªŒï¼‰
def _write_ocr_results(pairs, event):
    if not pairs:
        return "â— æœªå¯«å…¥ä»»ä½•è³‡æ–™ï¼ˆæœªæ‰¾åˆ°é…å°ï¼‰"
    
    ws = _ws(MAIN_SHEET_NAME)
    h = _get_header_map(ws)
    
    # è¨˜éŒ„è¡¨é ­è³‡è¨Š
    app.logger.info(f"[OCR_DEBUG] Header map: {h}")
    
    idxA = _col_idx(h, "ç´€éŒ„ID", 1)
    idxJ = _col_idx(h, "å¯„å‡ºæ—¥æœŸ", 10)
    idxK = _col_idx(h, "è¨—é‹å–®è™Ÿ", 11)
    idxL = _col_idx(h, "ç¶“æ‰‹äºº", 12)
    idxM = _col_idx(h, "å¯„é€ç‹€æ…‹", 13)
    idxD = _col_idx(h, "å­¸å“¡å§“å", 4)  # ç”¨æ–¼é™¤éŒ¯

    try:
        profile = line_bot_api.get_profile(event.source.user_id)
        uploader = profile.display_name or "LINEä½¿ç”¨è€…"
    except Exception:
        uploader = "LINEä½¿ç”¨è€…"

    all_vals = ws.get_all_values()
    app.logger.info(f"[OCR_DEBUG] Total rows in sheet: {len(all_vals)}")
    
    rows = all_vals[1:]  # è·³éè¡¨é ­

    # â˜…â˜… æ”¹ç”¨æ›´å®‰å…¨çš„æ–¹å¼å»ºç«‹ RID æ˜ å°„
    id2rows = {}
    for ridx, r in enumerate(rows, start=2):
        try:
            # ç¢ºä¿æœ‰è¶³å¤ çš„æ¬„ä½
            if len(r) < idxA:
                app.logger.warning(f"[OCR_DEBUG] Row {ridx} has insufficient columns: {len(r)}")
                continue
                
            rid = (r[idxA-1] or "").strip()
            if re.fullmatch(r"R\d{4}", rid):
                # è¨˜éŒ„æ‰¾åˆ°çš„RIDå’Œå°æ‡‰è¡Œè™Ÿã€å§“åï¼ˆç”¨æ–¼é™¤éŒ¯ï¼‰
                student_name = r[idxD-1] if len(r) >= idxD else "N/A"
                app.logger.info(f"[OCR_DEBUG] Found RID {rid} at row {ridx}, student: {student_name}")
                id2rows.setdefault(rid, []).append(ridx)
        except Exception as e:
            app.logger.error(f"[OCR_DEBUG] Error processing row {ridx}: {e}")
            continue

    app.logger.info(f"[OCR_DEBUG] RID mapping: {id2rows}")

    updated = []
    for rid, no in pairs:
        row_is = id2rows.get(rid, [])
        app.logger.info(f"[OCR_DEBUG] Updating RID {rid} with number {no} at rows: {row_is}")
        
        if not row_is:
            app.logger.warning(f"[OCR_DEBUG] RID {rid} not found in sheets")
            continue
            
        # å°åŒä¸€RIDçš„æ‰€æœ‰åˆ—éƒ½å¯«å…¥ç›¸åŒçš„å‡ºè²¨è³‡è¨Š
        for row_i in row_is:
            try:
                # é€ä¸€æ›´æ–°æ¯å€‹æ¬„ä½ä¸¦è¨˜éŒ„
                app.logger.info(f"[OCR_DEBUG] Updating row {row_i}: setting tracking number to {no}")
                ws.update_cell(row_i, idxK, f"'{no}")
                
                app.logger.info(f"[OCR_DEBUG] Updating row {row_i}: setting ship date to {today_str()}")
                ws.update_cell(row_i, idxJ, today_str())
                
                app.logger.info(f"[OCR_DEBUG] Updating row {row_i}: setting handler to {uploader}")
                ws.update_cell(row_i, idxL, uploader)
                
                app.logger.info(f"[OCR_DEBUG] Updating row {row_i}: setting status to å·²è¨—é‹")
                ws.update_cell(row_i, idxM, "å·²è¨—é‹")
                
            except Exception as e:
                app.logger.error(f"[OCR_DEBUG] Error updating row {row_i}: {e}")
                continue
                
        updated.append((rid, no))

    if not updated:
        return "â— æœªå¯«å…¥ï¼ˆæ‰¾ä¸åˆ°å°æ‡‰çš„ç´€éŒ„IDï¼‰"

    lines = [f"{rid} â†’ {no}" for rid, no in updated]
    result_msg = "âœ… å·²æ›´æ–°ï¼š{} ç­†\n{}".format(len(updated), "\n".join(lines))
    app.logger.info(f"[OCR_DEBUG] Final result: {result_msg}")
    return result_msg

# ============================================
# åŠŸèƒ½ Hï¼šå…¥åº«ï¼ˆ#è²·æ›¸ / #å…¥åº«ï¼›æ”¯æ´è² æ•¸ï¼ç›¤é»èª¿æ•´ï¼‰
# ã€Šå…¥åº«æ˜ç´°ã€‹è¡¨é ­ï¼šæ—¥æœŸ/ç¶“æ‰‹äºº/æ›¸ç±åç¨±/æ•¸é‡/ä¾†æº/å‚™è¨»
# ============================================
def _ensure_stockin_sheet():
    return _get_or_create_ws(STOCK_IN_SHEET_NAME, ["æ—¥æœŸ","ç¶“æ‰‹äºº","æ›¸ç±åç¨±","æ•¸é‡","ä¾†æº","å‚™è¨»"])

def _parse_stockin_text(body: str):
    """
    é€è¡Œè§£æã€‚æ¯è¡ŒæŠ“æœ€å¾Œä¸€å€‹æ•´æ•¸(å¯å« +/-)ä½œç‚ºæ•¸é‡ï¼›æ‰¾ä¸åˆ°å‰‡é è¨­ 1ã€‚
    æ›¸åï¼šå»é™¤æ•¸é‡å¾Œé€ resolve_book_nameã€‚
    å›å‚³ï¼šitems=[{"name":æ›¸å,"qty":æ•¸é‡}], errors=[str], ambiguous=[(raw,[å€™é¸])]
    """
    lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
    items, errors, ambiguous = [], [], []
    for ln in lines:
        # æ”¯æ´ +/-ï¼šx -3 / Ã— -3 / * -3 / çµå°¾ -3 / æ•¸é‡ï¼š-3
        m = re.search(r"(?:x|Ã—|\*)\s*([+-]?\d+)$", ln, re.I)
        if not m:
            m = re.search(r"([+-]?\d+)\s*(æœ¬|å¥—|å†Š)?$", ln)
        if not m:
            m = re.search(r"æ•¸é‡[:ï¼š]\s*([+-]?\d+)", ln)

        qty = int(m.group(1)) if m else 1

        # å»æ‰å°¾ç«¯æ•¸é‡ç‰‡æ®µ
        title = ln
        if m:
            title = ln[:m.start()].strip()

        # æ¸…ç†é€£æ¥ç¬¦
        title = re.sub(r"[ï¼š:\-â€“â€”]+$", "", title).strip()

        book, kind, extra = resolve_book_name(title)
        if not book:
            if kind == "ambiguous" and extra:
                ambiguous.append((ln, extra[:10]))
            else:
                errors.append(f"æ‰¾ä¸åˆ°æ›¸åï¼š{ln}")
            continue
        items.append({"name": book, "qty": qty})
    if not lines:
        errors.append("æ²’æœ‰è®€åˆ°ä»»ä½•å…§å®¹")
    return items, errors, ambiguous

def _handle_stockin(event, text):
    try:
        profile = line_bot_api.get_profile(event.source.user_id)
        operator = profile.display_name or "LINEä½¿ç”¨è€…"
    except Exception:
        operator = "LINEä½¿ç”¨è€…"

    body = re.sub(r"^#(è²·æ›¸|å…¥åº«)\s*", "", text.strip())
    items, errs, amb = _parse_stockin_text(body)

    if amb:
        tips = []
        for raw, choices in amb:
            tips.append(f"ã€Œ{raw}ã€å¯èƒ½æ˜¯ï¼š{ 'ã€'.join(choices) }")
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="â— æ›¸åä¸å¤ æ˜ç¢ºï¼š\n" + "\n".join(tips)))
        return
    if errs:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ å…¥åº«è³‡æ–™æœ‰èª¤ï¼š\n- " + "\n- ".join(errs)))
        return
    if not items:
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âŒ æ²’æœ‰å¯å…¥åº«çš„é …ç›®"))
        return

    # åˆä½µç›¸åŒæ›¸å
    merged = {}
    for it in items:
        merged[it["name"]] = merged.get(it["name"], 0) + int(it["qty"])
    items = [{"name": k, "qty": v} for k, v in merged.items()]

    has_negative = any(it["qty"] < 0 for it in items)

    # å­˜å…¥ pending ç­‰ç¢ºèª
    _PENDING[event.source.user_id] = {
        "type": "stock_in_confirm",
        "operator": operator,
        "items": items
    }
    lines = [f"â€¢ {it['name']} Ã— {it['qty']}" for it in items]
    suffix = "\n\nâ€» å«è² æ•¸ï¼ˆè‡ªå‹•æ¨™ç¤ºä¾†æºï¼šç›¤é»èª¿æ•´ï¼‰" if has_negative else ""
    msg = "è«‹ç¢ºèªå…¥åº«é …ç›®ï¼š\n" + "\n".join(lines) + suffix + "\n\nå›è¦†ã€ŒOK / YES / Yã€ç¢ºèªï¼›æˆ–å›è¦†ã€ŒNã€å–æ¶ˆã€‚"
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=msg))

def _write_stockin_rows(operator: str, items: list[dict]):
    ws = _ensure_stockin_sheet()
    rows = []
    for it in items:
        qty = int(it["qty"])
        source = "è³¼è²·" if qty >= 0 else "ç›¤é»èª¿æ•´"
        rows.append([today_str(), operator, it["name"], qty, source, ""])
    ws.append_rows(rows, value_input_option="USER_ENTERED")

# ============================================
# åŠŸèƒ½ Iï¼‹å…±ç”¨ï¼šè™•ç†å¾…ç¢ºèªå›ç­”ï¼ˆY/N/YES/OKï¼‰
# ï¼ˆå«ï¼šå–æ¶ˆå¯„æ›¸å¤šåˆ—åŒIDä¸€æ¬¡åˆªï¼‰
# ============================================
def _handle_pending_answer(event, text):
    pend = _PENDING.get(event.source.user_id)
    if not pend: return False
    ans = text.strip().upper()
    if ans not in ("Y","N","YES","OK"):
        return False
    if ans in ("N",):
        _PENDING.pop(event.source.user_id, None)
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="å·²å–æ¶ˆã€‚"))
        return True

    # YES / OK / Y
    if pend["type"] == "cancel_order":
        ws = _ws(pend["sheet"])
        idxH = pend["idx"]["H"]
        idxL = pend["idx"]["L"]
        idxM = pend["idx"]["M"]

        append_note = f"[å·²åˆªé™¤ {now_str_min()}]"
        for row_i in sorted(pend["rows"], reverse=False):
            try:
                curr_h = ws.cell(row_i, idxH).value or ""
            except Exception:
                curr_h = ""
            new_h = (curr_h + " " + append_note).strip() if curr_h else append_note
            ws.update_cell(row_i, idxH, new_h)
            ws.update_cell(row_i, idxL, pend["operator"])
            ws.update_cell(row_i, idxM, "å·²åˆªé™¤")

        msg = f"âœ… å·²åˆªé™¤æ•´ç­†å¯„æ›¸ï¼ˆ{pend['rid']}ï¼‰ï¼š{pend['stu']} çš„ {pend['book_list']}"
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=msg))
        _PENDING.pop(event.source.user_id, None)
        return True

    if pend["type"] == "stock_in_confirm":
        _write_stockin_rows(pend["operator"], pend["items"])
        lines = [f"{it['name']} Ã— {it['qty']}" for it in pend["items"]]
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text="âœ… å…¥åº«å®Œæˆï¼š\n" + "\n".join(lines)))
        _PENDING.pop(event.source.user_id, None)
        return True

    if pend["type"] == "æ•´ç†å¯„æ›¸_confirm":
        # å°‡è³‡æ–™è½‰æ›æˆ #å¯„æ›¸ æ ¼å¼æ–‡å­—ï¼Œäº¤çµ¦æ—¢æœ‰çš„ _handle_new_order
        data = pend["data"]
        fake_text = (
            "#å¯„æ›¸\n"
            f"å§“åï¼š{data['name']}\n"
            f"é›»è©±ï¼š{data['phone']}\n"
            f"å¯„é€åœ°å€ï¼š{data['address']}\n"
            f"æ›¸ç±åç¨±ï¼š{data['book_raw']}\n"
            f"æ¥­å‹™å‚™è¨»ï¼š{data['biz_note']}"
        )
        _handle_new_order(event, fake_text)
        _PENDING.pop(event.source.user_id, None)
        return True

    if pend["type"] == "æ•´ç†å¯„æ›¸_confirm":
        data = pend["data"]
        book_raw = data.get("book_raw") or ""
        fake_text = (
            "#å¯„æ›¸\n"
            f"å§“åï¼š{data.get('name') or ''}\n"
            f"é›»è©±ï¼š{data.get('phone') or ''}\n"
            f"å¯„é€åœ°å€ï¼š{data.get('address') or ''}\n"
            f"æ›¸ç±åç¨±ï¼š{book_raw}\n"
            f"æ¥­å‹™å‚™è¨»ï¼š{data.get('biz_note') or ''}"
        )
        _handle_new_order(event, fake_text)
        _PENDING.pop(event.source.user_id, None)
        return True

    return False

# ============================================
# LINE Webhook
# ============================================
@app.route("/callback", methods=["POST"])
def callback():
    signature = request.headers.get("X-Line-Signature", "")
    body = request.get_data(as_text=True)
    app.logger.info("Request body: " + body)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return "OK"

# æ–‡å­—è¨Šæ¯è™•ç†ï¼ˆåŠŸèƒ½ Jï¼šåªè™•ç†æŒ‡å®šæŒ‡ä»¤ï¼‰
@handler.add(MessageEvent, message=TextMessage)
def handle_text_message(event):
    text = (event.message.text or "").strip()

    # åŠŸèƒ½ Iï¼š#æˆ‘çš„IDï¼ˆä¸å—ç™½åå–®é™åˆ¶ï¼‰
    if text.startswith("#æˆ‘çš„ID"):
        uid = getattr(event.source, "user_id", "")
        try:
            profile = line_bot_api.get_profile(uid)
            name = profile.display_name or "LINEä½¿ç”¨è€…"
        except Exception:
            name = "LINEä½¿ç”¨è€…"
        try:
            line_bot_api.reply_message(
                event.reply_token,
                TextSendMessage(text=f"ä½ çš„ IDï¼š\n{uid}\né¡¯ç¤ºåç¨±ï¼š{name}\n\nè«‹æä¾›çµ¦ç®¡ç†å“¡åŠ å…¥ç™½åå–®ã€‚")
            )
        except Exception:
            pass
        if uid:
            _log_candidate(uid, name)
        return

    # å…±ç”¨ï¼šå¾…ç¢ºèªæµç¨‹ï¼ˆY/N/YES/OKï¼‰
    if _handle_pending_answer(event, text):
        return

    # ç™½åå–®æª¢æŸ¥ï¼ˆå…¶é¤˜æŒ‡ä»¤éƒ½è¦ï¼‰
    if not _ensure_authorized(event, scope="text"):
        return

    # åƒ…è™•ç†ä»¥ä¸‹æŒ‡ä»¤ï¼›å…¶é¤˜ç›´æ¥ä¸å›è¦†

    if text.startswith("#æ•´ç†å¯„æ›¸"):
        _handleæ•´ç†å¯„æ›¸(event, text); return
    
    if text.startswith("#å¯„æ›¸"):
        _handle_new_order(event, text); return

    if text.startswith("#æŸ¥è©¢å¯„æ›¸") or text.startswith("#æŸ¥å¯„æ›¸"):
        _handle_query(event, text); return

    if text.startswith("#å–æ¶ˆå¯„æ›¸") or text.startswith("#åˆªé™¤å¯„æ›¸"):
        _handle_cancel_request(event, text); return

    if text.startswith("#åˆªé™¤å‡ºæ›¸") or text.startswith("#å–æ¶ˆå‡ºæ›¸"):
        _handle_delete_ship(event, text); return

    if text.startswith("#å‡ºæ›¸"):
        # é–‹å•Ÿ OCR è¦–çª—
        _start_ocr_session(getattr(event.source, "user_id", ""))
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=f"å·²å•Ÿç”¨å‡ºæ›¸OCRï¼ˆ{OCR_SESSION_TTL_MIN} åˆ†é˜ï¼‰ã€‚è«‹ä¸Šå‚³å‡ºè²¨å–®ç…§ç‰‡ã€‚"))
        return

    if text.startswith("#è²·æ›¸") or text.startswith("#å…¥åº«"):
        _handle_stockin(event, text); return

    # å…¶ä»–æ–‡å­—ï¼šä¸è™•ç†ã€ä¸å›è¦†
    return

# åœ–ç‰‡è¨Šæ¯è™•ç†ï¼ˆåŠŸèƒ½ Eï¼šåƒ…åœ¨ #å‡ºæ›¸ å¾Œ N åˆ†é˜å…§æ‰å•Ÿç”¨ï¼‰
@handler.add(MessageEvent, message=ImageMessage)
def handle_image_message(event):
    # ç™½åå–®ï¼šæœªæˆæ¬Šç›´æ¥æ“‹ï¼ˆåœ–ç‰‡ä¸å›è¦†ï¼‰
    if not _ensure_authorized(event, scope="image"):
        return

    uid = getattr(event.source, "user_id", "")
    if not _has_ocr_session(uid):
        # æœªå…ˆ #å‡ºæ›¸ æˆ–å·²é€¾æ™‚ï¼šä¸è™•ç†ã€ä¸å›è¦†
        return

    try:
        app.logger.info(f"[IMG] æ”¶åˆ°åœ–ç‰‡ user_id={uid} msg_id={event.message.id}")
        img_bytes = _download_line_image_bytes(event.message.id)
        if not _vision_client:
            # æœ‰å•Ÿå‹•OCRæœƒè©±ï¼Œä½† Vision æœªè¨­å®š â†’ å›è¦†éŒ¯èª¤
            line_bot_api.reply_message(
                event.reply_token,
                TextSendMessage(text="âŒ OCR éŒ¯èª¤ï¼šVision æœªåˆå§‹åŒ–ï¼ˆè«‹è¨­å®š GOOGLE_SERVICE_ACCOUNT_JSON ä¸¦å•Ÿç”¨ Vision APIï¼‰ã€‚")
            )
            return

        text = _ocr_text_from_bytes(img_bytes)
        if LOG_OCR_RAW:
            app.logger.info(f"[OCR_TEXT]\n{text}")

        pairs, leftovers = _pair_ids_with_numbers(text)
        resp = _write_ocr_results(pairs, event)
        if leftovers:
            resp += "\n\nâ—ä»¥ä¸‹é …ç›®éœ€äººå·¥æª¢æ ¸ï¼š\n" + "\n".join(leftovers[:10])

        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=resp))
    except Exception as e:
        code = datetime.now(TZ).strftime("%Y%m%d%H%M%S")
        app.logger.exception("[OCR_ERROR]")
        try:
            line_bot_api.reply_message(event.reply_token, TextSendMessage(text=f"âŒ OCR éŒ¯èª¤ï¼ˆä»£ç¢¼ {code}ï¼‰ï¼š{e}"))
        except Exception:
            pass
    finally:
        # å–®æ¬¡è™•ç†å¾Œé—œé–‰æœƒè©±ï¼›å¦‚éœ€å¤šå¼µï¼Œå†è¼¸å…¥ #å‡ºæ›¸
        _clear_ocr_session(uid)

# å¥åº·æª¢æŸ¥
@app.route("/", methods=["GET"])
def index():
    try:
        names = [ws.title for ws in ss.worksheets()]
        return "OK / Worksheets: " + ", ".join(names)
    except Exception as e:
        return f"OK / (Sheets not loaded) {e}"

# æœ¬åœ°åŸ·è¡Œï¼ˆRailway ç”¨ gunicorn å•Ÿå‹•ï¼‰
if __name__ == "__main__":
    port = int(os.getenv("PORT", "8080"))
    app.run(host="0.0.0.0", port=port)
